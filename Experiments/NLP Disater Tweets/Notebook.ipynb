{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from nltk import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Paths\n",
    "\n",
    "These paths assume that the code is running from the location where this notebook is saved. If you want to run from a different location change the paths accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"Data/\"\n",
    "nltk_path = \"NLTK/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_path+'train.csv')\n",
    "test = pd.read_csv(data_path+'test.csv')\n",
    "\n",
    "#Mix the data so that real and fake accounts are not clustered\n",
    "train = train.reindex(np.random.permutation(train.index)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\",\n",
    "                       \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\",\n",
    "                       \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\n",
    "                       \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \n",
    "                       \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n",
    "                       \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                       \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\",\n",
    "                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\",\n",
    "                       \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\",\n",
    "                       \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\",\n",
    "                       \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n",
    "                       \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n",
    "                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                       \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
    "                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
    "                       \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\",\n",
    "                       \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
    "                       \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
    "                       \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\",\n",
    "                       \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                       \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                       \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                       \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                       \"you'd\": \"you would\", \"you'd've\": \"you would have\",\n",
    "                       \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                       \"you're\": \"you are\", \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_textnum(text):\n",
    "    '''\n",
    "    To seperate numbers from the words.\n",
    "    \n",
    "    Input - Word\n",
    "    \n",
    "    Returns - Number seperated list of items.\n",
    "    '''\n",
    "    match = re.match(r\"([a-z]+)([0-9]+)\", text, re.I)\n",
    "    if match:\n",
    "        items = \" \".join(list(match.groups()))\n",
    "    else:\n",
    "        match = re.match(r\"([0-9]+)([a-z]+)\", text, re.I)\n",
    "        if match:\n",
    "            items = \" \".join(list(match.groups()))\n",
    "        else:\n",
    "            return text\n",
    "    return (items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove special charecters and unwanted datas. This is totally manual task, kind of real pain of NLP data cleaning.\n",
    "def clean_text(text): \n",
    "            \n",
    "    # Special characters\n",
    "    text = re.sub(r\"%20\", \" \", text)\n",
    "    #text = text.replace(r\".\", \" \")\n",
    "    text = text.replace(r\"@\", \" \")\n",
    "    text = text.replace(r\"#\", \" \")\n",
    "    #text = text.replace(r\":\", \" \")\n",
    "    text = text.replace(r\"'\", \" \")\n",
    "    text = text.replace(r\"\\x89û_\", \" \")\n",
    "    text = text.replace(r\"??????\", \" \")\n",
    "    text = text.replace(r\"\\x89ûò\", \" \")\n",
    "    text = text.replace(r\"16yr\", \"16 year\")\n",
    "    text = text.replace(r\"re\\x89û_\", \" \")\n",
    "    \n",
    "    text = text.replace(r\"mh370\", \" \")\n",
    "    text = text.replace(r\"prebreak\", \"pre break\")\n",
    "    text = re.sub(r\"\\x89û\", \" \", text)\n",
    "    text = re.sub(r\"re\\x89û\", \"re \", text)\n",
    "    text = text.replace(r\"nowplaying\", \"now playing\")\n",
    "    text = re.sub(r\"\\x89ûª\", \"'\", text)\n",
    "    text = re.sub(r\"\\x89û\", \" \", text)\n",
    "    text = re.sub(r\"\\x89ûò\", \" \", text)\n",
    "    \n",
    "    \n",
    "    text = re.sub(r\"\\x89Û_\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÒ\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÓ\", \"\", text)\n",
    "    text = re.sub(r\"\\x89ÛÏWhen\", \"When\", text)\n",
    "    text = re.sub(r\"\\x89ÛÏ\", \"\", text)\n",
    "    text = re.sub(r\"China\\x89Ûªs\", \"China's\", text)\n",
    "    text = re.sub(r\"let\\x89Ûªs\", \"let's\", text)\n",
    "    text = re.sub(r\"\\x89Û÷\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Ûª\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û\\x9d\", \"\", text)\n",
    "    text = re.sub(r\"å_\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û¢\", \"\", text)\n",
    "    text = re.sub(r\"\\x89Û¢åÊ\", \"\", text)\n",
    "    text = re.sub(r\"fromåÊwounds\", \"from wounds\", text)\n",
    "    text = re.sub(r\"åÊ\", \"\", text)\n",
    "    text = re.sub(r\"åÈ\", \"\", text)\n",
    "    text = re.sub(r\"JapÌ_n\", \"Japan\", text)    \n",
    "    text = re.sub(r\"Ì©\", \"e\", text)\n",
    "    text = re.sub(r\"å¨\", \"\", text)\n",
    "    text = re.sub(r\"SuruÌ¤\", \"Suruc\", text)\n",
    "    text = re.sub(r\"åÇ\", \"\", text)\n",
    "    text = re.sub(r\"å£3million\", \"3 million\", text)\n",
    "    text = re.sub(r\"åÀ\", \"\", text)\n",
    "    \n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    text = re.sub(r\"ªs\", \" \", text)\n",
    "    text = re.sub(r\"ª\", \" \", text)\n",
    "    text = re.sub(r\"\\x9d\", \" \", text)\n",
    "    text = re.sub(r\"ò\", \" \", text)\n",
    "    text = re.sub(r\"ªt\", \" \", text)\n",
    "    text = re.sub(r\"ó\", \" \", text)\n",
    "    text = text.replace(r\"11yearold\", \"11 year old\")\n",
    "    text = re.sub(r\"typhoondevastated\", \"typhoon devastated\", text)\n",
    "    text = re.sub(r\"bestnaijamade\", \"best nijamade\", text)\n",
    "    text = re.sub(r\"gbbo\", \"The Great British Bake Off\", text)\n",
    "    text = re.sub(r\"ï\", \"\", text)\n",
    "    text = re.sub(r\"ïwhen\", \"when\", text)\n",
    "    text = re.sub(r\"selfimage\", \"self image\", text)\n",
    "    text = re.sub(r\"20150805\", \"2015 08 05\", text)\n",
    "    text = re.sub(r\"20150806\", \"2015 08 06\", text)\n",
    "    text = re.sub(r\"subreddits\", \"website for weird public sentiment\", text)\n",
    "    text = re.sub(r\"disea\", \"chinese famous electronic company\", text)\n",
    "    text = re.sub(r\"lmao\", \"funny\", text)\n",
    "    text = text.replace(r\"companyse\", \"company\")\n",
    "    \n",
    "    text = text.replace(r\"worldnews\", \"world news\")\n",
    "    text = text.replace(r\"animalrescue\", \"animal rescue\")\n",
    "    text = text.replace(r\"freakiest\", \"freak\")\n",
    "    \n",
    "    text = text.replace(r\"irandeal\", \"iran deal\")\n",
    "    text = text.replace(r\"directioners\", \"mentor\")\n",
    "    text = text.replace(r\"justinbieber\", \"justin bieber\")\n",
    "    text = text.replace(r\"okwx\", \"okay\")\n",
    "    text = text.replace(r\"trapmusic\", \"trap music\")\n",
    "    text = text.replace(r\"djicemoon\", \"music ice moon\")\n",
    "    text = text.replace(r\"icemoon\", \"ice moon\")\n",
    "    text = text.replace(r\"mtvhottest\", \"tv hottest\")\n",
    "    text = text.replace(r\"rì©union\", \"reunion\")\n",
    "    text = text.replace(r\"abcnews\", \"abc news\")\n",
    "    text = text.replace(r\"tubestrike\", \"tube strike\")\n",
    "    text = text.replace(r\"prophetmuhammad\", \"prophet muhammad muslim dharma\")\n",
    "    text = text.replace(r\"chicagoarea\", \"chicago area\")\n",
    "    text = text.replace(r\"yearold\", \"year old\")\n",
    "    text = text.replace(r\"meatloving\", \"meat love\")\n",
    "    text = text.replace(r\"standuser\", \"standard user\")\n",
    "    text = text.replace(r\"pantherattack\", \"panther attack\")\n",
    "    text = text.replace(r\"youngheroesid\", \"young hearos id\")\n",
    "    text = text.replace(r\"idk\", \"i do not know\")\n",
    "    text = text.replace(r\"usagov\", \"united state of america government\")\n",
    "    text = text.replace(r\"injuryi\", \"injury\")\n",
    "    text = text.replace(r\"summerfate\", \"summer fate\")\n",
    "    text = text.replace(r\"kerricktrial\", \"kerrick trial\")\n",
    "    text = text.replace(r\"viralspell\", \"viral spell\")\n",
    "    text = text.replace(r\"collisionno\", \"collision\")\n",
    "    text = text.replace(r\"socialnews\", \"social news\")\n",
    "    text = text.replace(r\"nasahurricane\", \"nasa hurricane\")\n",
    "    text = text.replace(r\"strategicpatience\", \"strategic patience\")\n",
    "    text = text.replace(r\"explosionproof\", \"explosion proof\")\n",
    "    text = text.replace(r\"selfies\", \"photo\")\n",
    "    text = text.replace(r\"selfie\", \"photo\")\n",
    "    text = text.replace(r\"worstsummerjob\", \"worst summer job\")\n",
    "    text = text.replace(r\"realdonaldtrump\", \"real america president\")\n",
    "    text = text.replace(r\"omfg\", \"oh my god\")\n",
    "    text = text.replace(r\"japìn\", \"japan\")\n",
    "    text = text.replace(r\"breakingnews\", \"breaking news\")\n",
    "    \n",
    "    text = \" \".join([split_textnum(word) for word in text.split(\" \")])\n",
    "    \n",
    "    text = \"\".join([c if c not in string.punctuation else \"\" for c in text])\n",
    "    text = ''.join(c for c in text if not c.isdigit())\n",
    "    text = text.replace(r\"÷\", \"\")\n",
    "    \n",
    "    text = re.sub(' +', ' ', text)\n",
    "    # text = text.encode('utf-8')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text_processed'] = train['text'].apply(lambda x : \" \".join([contraction_mapping[word].lower() \n",
    "                    if word in contraction_mapping.keys() else word.lower() for word in x.split(\" \")]))\n",
    "test['text_processed'] = test['text'].apply(lambda x : \" \".join([contraction_mapping[word].lower() \n",
    "                    if word in contraction_mapping.keys() else word.lower() for word in x.split(\" \")]))\n",
    "train['text_processed'] = train['text_processed'].apply(lambda x : clean_text(x))\n",
    "test['text_processed'] = test['text_processed'].apply(lambda x : clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append(nltk_path + \"nltk_data/\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "train['text_processed'] = train['text_processed'].apply(lambda x : \"\".join([lemmatizer.lemmatize(word) \n",
    "                                                                            for word in x]))\n",
    "test['text_processed'] = test['text_processed'].apply(lambda x : \"\".join([lemmatizer.lemmatize(word) \n",
    "                                                                          for word in x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(data):\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    emb = count_vectorizer.fit_transform(data)\n",
    "    return emb, count_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(data):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    train = tfidf_vectorizer.fit_transform(data)\n",
    "    return train, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_corpus = train[\"text_processed\"].tolist()\n",
    "list_labels = train[\"target\"].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of Words embeddings\n",
    "X_train_counts, count_vectorizer = cv(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "#TF-IDF embeddings\n",
    "X_train_tfidf, tfidf_vectorizer = tfidf(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPdisaster",
   "language": "python",
   "name": "nlpdisaster"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
